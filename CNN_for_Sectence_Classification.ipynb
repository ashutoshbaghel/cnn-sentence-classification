{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN for Sectence Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "sN-dk4y9ss7s"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODuYobXlEEYlYjs9inZI0t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashutoshbaghel/cnn-sentence-classification/blob/master/CNN_for_Sectence_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOzzeUKZsxiG",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9ifPfp2XfIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rIFWmtYTq60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "0f468aae-4113-427e-c0c2-0f6821702492"
      },
      "source": [
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils as utils\n",
        "import pickle as pk\n",
        "from torch.utils.data import DataLoader, Dataset \n",
        "import time\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "print(torch.cuda.get_device_name())"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "Tesla T4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXVex4iUmsSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def append_to_file(my_list, fname=\"output.txt\"):\n",
        "    with open(fname, \"w\") as f:\n",
        "        if(type(my_list)==list):\n",
        "            for item in my_list:\n",
        "                f.write(\"%s\\n\" % item)\n",
        "        elif (type(my_list)==str):\n",
        "            f.write(\"%s\\n\" % my_list)\n",
        "        else:\n",
        "            print(\"[ERROR] Only writes lists and texts to file.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sN-dk4y9ss7s",
        "colab_type": "text"
      },
      "source": [
        "# Get files in working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd9StFgZKapH",
        "colab_type": "text"
      },
      "source": [
        "All files are also downloadable from [Google Drive](https://drive.google.com/drive/folders/1G_AZqP5zOVRGaLe2MqE2OJVQ6xUkdhpL?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TFq8mFAjvFL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b8300ad7-7d50-403f-e181-2d1ba5d3812e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "# %cd /gdrive"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj2e9zHTQ1UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -a \"/gdrive/My Drive/data/.\" ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E28z2wSGtPd3",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "941B155ppHyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_str(string):\n",
        "    \"\"\"\n",
        "    String cleaning and tokenization for dataset\n",
        "    \"\"\"\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "    string = re.sub(r\",\", \" , \", string) \n",
        "    string = re.sub(r\"!\", \" ! \", string) \n",
        "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
        "    return string.strip().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64BtWqMIWu9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_data = np.genfromtxt(\"topicclass_valid.txt\", delimiter=\"|||\", dtype=\"str\")\n",
        "train_data = np.genfromtxt(\"topicclass_train.txt\", delimiter=\"|||\", dtype=\"str\")\n",
        "test_data = np.genfromtxt(\"topicclass_test.txt\", delimiter=\"|||\", dtype=\"str\")\n",
        "assert len(train_data)==253909\n",
        "assert len(val_data)==643\n",
        "assert len(test_data)==697"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJoEphI1viQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label = train_data[:,0]\n",
        "train_sent = [clean_str(sent) for sent in train_data[:,1]]\n",
        "val_label = val_data[:,0]\n",
        "val_sent = [clean_str(sent) for sent in val_data[:,1]]\n",
        "test_label = test_data[:,0]\n",
        "test_sent = [clean_str(sent) for sent in test_data[:,1]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbAPQFLwQKu9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_label_idx_map(labels):\n",
        "    \"\"\"\n",
        "    takes list of labels from the train dataset\n",
        "    return dicts which map: label->index and index->label\n",
        "    \"\"\"\n",
        "    labels = set(labels)\n",
        "    i = 0\n",
        "    label_to_idx_map = {}\n",
        "    idx_to_label_map = {}\n",
        "    for label in labels:\n",
        "        label_to_idx_map[label.strip()] = i\n",
        "        idx_to_label_map[i] = label.strip()\n",
        "        i += 1\n",
        "    return label_to_idx_map, idx_to_label_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u87RT8F1QwJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "label_idx_map, idx_label_map = get_label_idx_map(train_label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAW_ezKszQSm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "504a3811-6987-4f28-ec9e-58c581684a1c"
      },
      "source": [
        "# Create vocab from train set\n",
        "vocab = defaultdict(float)\n",
        "for sent in train_sent:\n",
        "    for word in sent.split():\n",
        "        vocab[word] += 1\n",
        "print(len(vocab))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "112760\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSQr9P-R3IiW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load word2vec for words in vocab:\n",
        "def load_word2vec(fname, vocab):\n",
        "    word2vec = {}\n",
        "    with open(fname, 'r') as f:\n",
        "        header = f.readline()\n",
        "        vocab_size, layer1_size = map(int, header.split())\n",
        "        for i in range(vocab_size):\n",
        "            line = f.readline().split()\n",
        "            word, vec = line[0], np.asarray(line[1:], dtype=np.float32)\n",
        "            if word in vocab:\n",
        "                word2vec[word] = vec\n",
        "    return word2vec"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBf6fIzi4oEk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf791c4d-8adc-46e9-ccdb-358eef429b2a"
      },
      "source": [
        "word2vec = load_word2vec(\"./GoogleNews-vectors-negative300.txt\", vocab)\n",
        "len(vocab) - len(word2vec)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "59918"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfw4C1JU93wZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO\n",
        "# Can make this tinier, common for all unknown words \n",
        "def add_unknown_words(w2v, vocab):\n",
        "    i = 0 \n",
        "    for word in vocab:\n",
        "        if word not in w2v:\n",
        "            i +=1 \n",
        "            w2v[word] = np.random.uniform(-0.25,0.25,300) \n",
        "            #change this to other logic after seeing performace\n",
        "    print(\"Total unknown words= \", i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d8I2oq_-Rqb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a041189f-24cf-4561-e881-709ca68c1d8f"
      },
      "source": [
        "add_unknown_words(word2vec, vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total unknown words=  59918\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EkjgX00XAmMS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "81ff98f2-c0b4-45b1-b2e3-a77d22365ffc"
      },
      "source": [
        "len(word2vec) == len(vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWCmlVD1BVyG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_W(word_vecs, k=300):\n",
        "    \"\"\"\n",
        "    Get word matrix. W[i] is the vector for word indexed by i\n",
        "    \"\"\"\n",
        "    vocab_size = len(word_vecs)\n",
        "    word_idx_map = dict()\n",
        "    W = np.zeros(shape=(vocab_size+1, k), dtype=np.float32)\n",
        "    W[0] = np.zeros(k, dtype=np.float32)\n",
        "    i = 1\n",
        "    for word in word_vecs:\n",
        "        W[i] = word_vecs[word]\n",
        "        word_idx_map[word] = i\n",
        "        i += 1\n",
        "    return W, word_idx_map"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pky37gA4Bd1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W, word_idx_map = get_W(word2vec)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9F9QlgMwB9VE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump([W, word_idx_map, label_idx_map, vocab], \n",
        "            open('w-wordmap-labelmap-vocab.pickle', 'wb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAvoQOJyMvNC",
        "colab_type": "text"
      },
      "source": [
        "# Prepare Train/Val/Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME40widXMzOP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_idx_from_sent(sent, word_idx_map):\n",
        "    \"\"\"\n",
        "    Transforms sentence into a list of indices. \n",
        "    \"\"\"\n",
        "    x = []\n",
        "    words = sent.split()\n",
        "    for word in words:\n",
        "        if word in word_idx_map:\n",
        "            x.append(word_idx_map[word])\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWB4LeY5NAeI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6dfca7e9-126c-4383-9593-32426873a4e2"
      },
      "source": [
        "train = [get_idx_from_sent(sent,word_idx_map) for sent in train_sent]\n",
        "train_label = [label_idx_map[label.strip()] for label in train_label]\n",
        "assert len(train) == len (train_label)\n",
        "print(len(train))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "253909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp_By3LGNas-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a1406699-1cde-4653-c9f1-4323ec0a11b0"
      },
      "source": [
        "val = [get_idx_from_sent(sent,word_idx_map) for sent in val_sent]\n",
        "val_label = [label_idx_map[label.strip()] for label in val_label]\n",
        "assert len(val) == len (val_label)\n",
        "print(len(val))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "643\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ItFFGlm5rJt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40271c7d-8447-4aa5-b2a7-71fa495070ca"
      },
      "source": [
        "test = [get_idx_from_sent(sent,word_idx_map) for sent in test_sent]\n",
        "test_label = np.zeros(len(test_sent))\n",
        "assert len(test) == len (test_label)\n",
        "print(len(test))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhooEG9vDV3T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, qualifier=\"final\"):\n",
        "    torch.save(model.state_dict(), f\"{model.__class__.__name__}-{qualifier}-{datetime.now().timestamp()}.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Xo-8W5KiIp5",
        "colab_type": "text"
      },
      "source": [
        "# Dataset and Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWS_trNkNyxz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sent, labels, W):\n",
        "        self.sentences = sent\n",
        "        self.labels = labels\n",
        "        self.W = W\n",
        "        self.word_idx_map = word_idx_map\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        sent = self.sentences[index]\n",
        "        vec = np.zeros((len(sent), 300))\n",
        "        for i,word_idx in enumerate(sent):\n",
        "            vec[i] = self.W[word_idx]\n",
        "        label = self.labels[index]\n",
        "        return torch.tensor(vec), torch.tensor(label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8gsXzUcXUJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def collate_train(batch_data):\n",
        "    '''\n",
        "    '''\n",
        "    X, Y = zip(*batch_data)\n",
        "    X = pad_sequence(X, batch_first=True)\n",
        "    return X, torch.tensor(Y)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAO2KOCmXKo8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = SentenceDataset(train, train_label, W)\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True, collate_fn=collate_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hh6L65p8Z0j0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dataset = SentenceDataset(val, val_label, W)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True, num_workers=8, pin_memory=True, collate_fn=collate_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsUMHIt_5g2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = SentenceDataset(test, test_label, W)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDmbmROMaFT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, output_size, in_channels, out_channels, kernel_size, stride, padding, dropout_prob, embed_size=300): #, vocab_size, embed_size, weights):\n",
        "        super(CNN, self).__init__()\n",
        "        \n",
        "        \"\"\"       \n",
        "        \"\"\"\n",
        "        self.output_size = output_size\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, (kernel_size[0], embed_size), stride, padding)\n",
        "        self.conv2 = nn.Conv2d(in_channels, out_channels, (kernel_size[1], embed_size), stride, padding)\n",
        "        self.conv3 = nn.Conv2d(in_channels, out_channels, (kernel_size[2], embed_size), stride, padding)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "        self.label = nn.Linear(len(kernel_size)*out_channels, output_size)\n",
        "    \n",
        "    def convolution(self, input, conv_layer):\n",
        "        out = conv_layer(input)# out.size() = (batch_size, out_channels, dim, 1)\n",
        "        out = F.relu(out.squeeze(3))# out.size() = (batch_size, out_channels, dim1)\n",
        "        max_output = F.max_pool1d(out, out.size()[2]).squeeze(2)# max_output.size() = (batch_size, out_channels)\n",
        "        return max_output\n",
        "    \n",
        "    def forward(self, sentences):\n",
        "        \n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        sentences: sentences of shape = (batch_size, num_sequences)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        logits.size() = (batch_size, output_size)\n",
        "        \"\"\"\n",
        "        \n",
        "        input = sentences.float() # input.size() = (batch_size, num_seq, embed_size)\n",
        "        input = input.unsqueeze(1)      # input.size() = (batch_size, 1, num_seq, embed_size)\n",
        "        max_output1 = self.convolution(input, self.conv1)\n",
        "        max_output2 = self.convolution(input, self.conv2)\n",
        "        max_output3 = self.convolution(input, self.conv3)\n",
        "        \n",
        "        out = torch.cat((max_output1, max_output2, max_output3), 1)  # out.size() = (batch_size, num_kernels*out_channels)\n",
        "        fc_in = self.dropout(out)  # fc_in.size() = (batch_size, num_kernels*out_channels)\n",
        "        logits = self.label(fc_in)\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuzZmah2dg1_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_size = 16\n",
        "input_channels = 1 # Corresponding to each sentence\n",
        "output_channels = 100\n",
        "kernel_size = [3,4,5]\n",
        "stride = 1\n",
        "padding = (2,0) # pad word-dimension, leave embedding size as is. \n",
        "dropout_prob = 0.5\n",
        "\n",
        "loss_fn = F.cross_entropy\n",
        "num_epoch = 10 \n",
        "\n",
        "mymodel = CNN(output_size, input_channels, output_channels, kernel_size, \n",
        "              stride, padding, dropout_prob)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88lXa8AxeH3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #TODO: Can add gradient clipping here. \n",
        " def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(model.parameters())\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    since = time.time()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        text = batch[0]\n",
        "        target = batch[1]\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            text = text.cuda()\n",
        "            target = target.cuda()\n",
        "        optim.zero_grad()\n",
        "        prediction = model(text)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        prediction = torch.max(prediction, 1)[1]\n",
        "        num_corrects = (prediction.data == target.data).sum()\n",
        "        acc = 100.0 * num_corrects/len(batch[0])\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        \n",
        "        if idx % 500 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.2f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "    end_time = time.time()    \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter), (end_time - since)\n",
        "\n",
        "def eval_model(model, val_iter, write_to_file=False, fname=\"output.txt\"):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            text = batch[0]\n",
        "            target = batch[1]\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                text = text.cuda() \n",
        "                target = target.cuda()\n",
        "            prediction = model(text)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            prediction = torch.max(prediction, 1)[1]\n",
        "            num_corrects = (prediction.data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch[0])\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "            \n",
        "            # write results to a file\n",
        "            if write_to_file:\n",
        "                append_to_file([idx_label_map[int(idx)] for idx in prediction], \n",
        "                               fname)\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2i_Ps6ve1mT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "6027624d-3b4d-4985-b04d-5b9da3991014"
      },
      "source": [
        "for epoch in range(num_epoch):\n",
        "    train_loss, train_acc, time_elapsed = train_model(mymodel, train_loader, epoch)\n",
        "    val_loss, val_acc = eval_model(mymodel, val_loader)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1}, Train Loss: {train_loss:.2f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:2f}, Val. Acc: {val_acc:.2f}%, , Time: {time_elapsed:.2f} sec')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, Idx: 1, Training Loss: 2.78, Training Accuracy:  5.47%\n",
            "Epoch: 1, Idx: 501, Training Loss: 1.02, Training Accuracy:  69.53%\n",
            "Epoch: 1, Idx: 1001, Training Loss: 0.91, Training Accuracy:  77.34%\n",
            "Epoch: 1, Idx: 1501, Training Loss: 1.00, Training Accuracy:  68.75%\n",
            "Epoch: 1, Train Loss: 1.01, Train Acc: 69.37%, Val. Loss: 0.640697, Val. Acc: 81.55%, , Time: 57.21\n",
            "Epoch: 2, Idx: 1, Training Loss: 0.96, Training Accuracy:  69.53%\n",
            "Epoch: 2, Idx: 501, Training Loss: 0.79, Training Accuracy:  78.12%\n",
            "Epoch: 2, Idx: 1001, Training Loss: 0.87, Training Accuracy:  73.44%\n",
            "Epoch: 2, Idx: 1501, Training Loss: 0.78, Training Accuracy:  81.25%\n",
            "Epoch: 2, Train Loss: 0.84, Train Acc: 73.96%, Val. Loss: 0.605385, Val. Acc: 81.25%, , Time: 56.80\n",
            "Epoch: 3, Idx: 1, Training Loss: 0.78, Training Accuracy:  75.00%\n",
            "Epoch: 3, Idx: 501, Training Loss: 0.80, Training Accuracy:  74.22%\n",
            "Epoch: 3, Idx: 1001, Training Loss: 0.86, Training Accuracy:  71.09%\n",
            "Epoch: 3, Idx: 1501, Training Loss: 0.68, Training Accuracy:  82.81%\n",
            "Epoch: 3, Train Loss: 0.79, Train Acc: 75.39%, Val. Loss: 0.593756, Val. Acc: 81.55%, , Time: 56.80\n",
            "Epoch: 4, Idx: 1, Training Loss: 0.65, Training Accuracy:  83.59%\n",
            "Epoch: 4, Idx: 501, Training Loss: 0.71, Training Accuracy:  75.00%\n",
            "Epoch: 4, Idx: 1001, Training Loss: 0.93, Training Accuracy:  77.34%\n",
            "Epoch: 4, Idx: 1501, Training Loss: 0.56, Training Accuracy:  84.38%\n",
            "Epoch: 4, Train Loss: 0.75, Train Acc: 76.47%, Val. Loss: 0.607654, Val. Acc: 81.81%, , Time: 56.45\n",
            "Epoch: 5, Idx: 1, Training Loss: 0.68, Training Accuracy:  75.78%\n",
            "Epoch: 5, Idx: 501, Training Loss: 0.71, Training Accuracy:  73.44%\n",
            "Epoch: 5, Idx: 1001, Training Loss: 0.75, Training Accuracy:  79.69%\n",
            "Epoch: 5, Idx: 1501, Training Loss: 0.83, Training Accuracy:  75.78%\n",
            "Epoch: 5, Train Loss: 0.72, Train Acc: 77.25%, Val. Loss: 0.592279, Val. Acc: 81.50%, , Time: 53.76\n",
            "Epoch: 6, Idx: 1, Training Loss: 0.95, Training Accuracy:  71.88%\n",
            "Epoch: 6, Idx: 501, Training Loss: 0.63, Training Accuracy:  80.47%\n",
            "Epoch: 6, Idx: 1001, Training Loss: 0.74, Training Accuracy:  76.56%\n",
            "Epoch: 6, Idx: 1501, Training Loss: 0.68, Training Accuracy:  83.59%\n",
            "Epoch: 6, Train Loss: 0.70, Train Acc: 77.97%, Val. Loss: 0.606572, Val. Acc: 82.83%, , Time: 53.85\n",
            "Epoch: 7, Idx: 1, Training Loss: 0.64, Training Accuracy:  80.47%\n",
            "Epoch: 7, Idx: 501, Training Loss: 0.58, Training Accuracy:  81.25%\n",
            "Epoch: 7, Idx: 1001, Training Loss: 0.63, Training Accuracy:  80.47%\n",
            "Epoch: 7, Idx: 1501, Training Loss: 0.67, Training Accuracy:  80.47%\n",
            "Epoch: 7, Train Loss: 0.67, Train Acc: 78.63%, Val. Loss: 0.583402, Val. Acc: 83.08%, , Time: 53.60\n",
            "Epoch: 8, Idx: 1, Training Loss: 0.53, Training Accuracy:  85.16%\n",
            "Epoch: 8, Idx: 501, Training Loss: 0.61, Training Accuracy:  80.47%\n",
            "Epoch: 8, Idx: 1001, Training Loss: 0.62, Training Accuracy:  79.69%\n",
            "Epoch: 8, Idx: 1501, Training Loss: 0.68, Training Accuracy:  78.91%\n",
            "Epoch: 8, Train Loss: 0.65, Train Acc: 79.13%, Val. Loss: 0.593832, Val. Acc: 82.16%, , Time: 56.60\n",
            "Epoch: 9, Idx: 1, Training Loss: 0.68, Training Accuracy:  82.03%\n",
            "Epoch: 9, Idx: 501, Training Loss: 0.77, Training Accuracy:  76.56%\n",
            "Epoch: 9, Idx: 1001, Training Loss: 0.62, Training Accuracy:  78.91%\n",
            "Epoch: 9, Idx: 1501, Training Loss: 0.84, Training Accuracy:  75.78%\n",
            "Epoch: 9, Train Loss: 0.64, Train Acc: 79.68%, Val. Loss: 0.618099, Val. Acc: 81.66%, , Time: 56.32\n",
            "Epoch: 10, Idx: 1, Training Loss: 0.51, Training Accuracy:  82.81%\n",
            "Epoch: 10, Idx: 501, Training Loss: 0.52, Training Accuracy:  85.16%\n",
            "Epoch: 10, Idx: 1001, Training Loss: 0.47, Training Accuracy:  86.72%\n",
            "Epoch: 10, Idx: 1501, Training Loss: 0.50, Training Accuracy:  81.25%\n",
            "Epoch: 10, Train Loss: 0.62, Train Acc: 80.12%, Val. Loss: 0.593598, Val. Acc: 82.93%, , Time: 54.68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdSoRKi6Gd7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model(mymodel)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtcDNUZqTnJR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93847781-1284-4dff-eef1-2583a0de64a8"
      },
      "source": [
        "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate_train)\n",
        "eval_model(mymodel, val_loader, write_to_file=True, fname=\"dev_results.txt\")"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6062538693796605, 81.95956454121307)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvzIW1-XyryL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7c02e450-7bb2-4c5c-cf8b-77d771f3864a"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=8, pin_memory=True, collate_fn=collate_train)\n",
        "eval_model(mymodel, test_loader, write_to_file=True, fname=\"test_results.txt\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8.164417405552639, 3.012912482065997)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqVpYzrOcwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}